# -*- coding: utf-8 -*-
"""FT-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s84Apz6Uts7YdzF0pwddE_UTIYuDn-nE
"""

!pip install transformers datasets accelerate evaluate
!pip install --upgrade transformers

import transformers, datasets
from datasets import load_dataset
from transformers import AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification

dataset = load_dataset("imdb")
print(dataset)

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def token_fun(row):
  return tokenizer(row["text"], truncation=True)

tokenized_dataset = dataset.map(token_fun, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"].shuffle(seed=42).select(range(1000)),
    eval_dataset=tokenized_dataset["test"].shuffle(seed=42).select(range(500)),
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()


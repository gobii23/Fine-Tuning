# -*- coding: utf-8 -*-
"""FT_IT(Auto).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JL4ZzeznKkzGc373kk0VbLvfmIjqd1la
"""

!pip install chromadb crewai crewai['tools'] datasets langchain-google-genai matplotlib pandas pydantic scikit-learn seaborn torch transformers trl tqdm unsloth

from crewai.tools import BaseTool
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    AutoModelForTokenClassification,
    Trainer,
    TrainingArguments,
)
from datasets import load_dataset
import os
import json


class FTTool(BaseTool):
    name: str = "FTTool"
    description: str = (
        "Fine-tunes a HuggingFace model for a given task type. "
        "Requires: model_name, dataset_name_or_path, task_type (causal_lm, sequence_classification, token_classification, etc.), "
        "and optional training arguments."
    )

    def _run(
        self, model_name: str, dataset_name_or_path: str, task_type: str, **kwargs
    ) -> str:
        try:
            model_loader_map = {
                "causal_lm": AutoModelForCausalLM,
                "sequence_classification": AutoModelForSequenceClassification,
                "token_classification": AutoModelForTokenClassification,
            }
            if task_type not in model_loader_map:
                return json.dumps(
                    {
                        "status": "error",
                        "message": f"Unsupported task_type '{task_type}'. Supported: {list(model_loader_map.keys())}",
                    },
                    indent=2,
                )

            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if task_type == "causal_lm" and tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            model = model_loader_map[task_type].from_pretrained(model_name)

            # Load dataset
            if os.path.exists(dataset_name_or_path):
                dataset = load_dataset("json", data_files=dataset_name_or_path)
            else:
                dataset = load_dataset(dataset_name_or_path)

            # Ensure validation split exists
            if "validation" not in dataset:
                if "test" in dataset:
                    dataset["validation"] = dataset["test"]
                elif "train" in dataset:
                    split_dataset = dataset["train"].train_test_split(test_size=0.1)
                    dataset["train"] = split_dataset["train"]
                    dataset["validation"] = split_dataset["test"]

            def tokenize_function(examples):
                max_len = kwargs.get(
                    "max_length", 512 if task_type == "causal_lm" else 256
                )
                return tokenizer(
                    examples["text"],
                    truncation=True,
                    padding="max_length",
                    max_length=max_len,
                )

            tokenized_datasets = dataset.map(tokenize_function, batched=True)

            # Adjust evaluation strategy
            eval_strategy = kwargs.get(
                "evaluation_strategy",
                "epoch" if "validation" in tokenized_datasets else "no",
            )

            output_dir = "./output/fine_tuning_results"
            train_args = TrainingArguments(
                output_dir=output_dir,
                eval_strategy=eval_strategy,
                learning_rate=kwargs.get("learning_rate", 5e-5),
                per_device_train_batch_size=kwargs.get("train_batch_size", 4),
                per_device_eval_batch_size=kwargs.get("eval_batch_size", 4),
                num_train_epochs=kwargs.get("num_epochs", 3),
                weight_decay=kwargs.get("weight_decay", 0.01),
                logging_dir=os.path.join(output_dir, "logs"),
                logging_steps=kwargs.get("logging_steps", 50),
                save_strategy="epoch",
                report_to=kwargs.get("report_to", "none"),
                push_to_hub=False,
            )

            trainer = Trainer(
                model=model,
                args=train_args,
                train_dataset=tokenized_datasets["train"],
                eval_dataset=tokenized_datasets.get("validation"),
                tokenizer=tokenizer,
            )

            trainer.train()

            model.save_pretrained(output_dir)
            tokenizer.save_pretrained(output_dir)

            return json.dumps(
                {
                    "status": "success",
                    "output_dir": output_dir,
                    "message": f"Fine-tuned {model_name} for {task_type} saved at {output_dir}",
                },
                indent=2,
            )

        except Exception as e:
            return json.dumps({"status": "error", "message": str(e)}, indent=2)






from crewai import Agent, Task, Crew, LLM
from dotenv import load_dotenv
import re
import os
import json
import pandas as pd
load_dotenv()

os.environ["OPENAI_API_KEY"] = "dummy"
os.environ["GEMINI_API_KEY"] = "AIzaSyAMP3_pGsIadNytYMbZL7nz6htVL5v8W30"

# LLM
llm = LLM(
    model="gemini/gemini-2.5-flash"
)


# TOOLS
ft_tool = FTTool()


# INPUT
ft_model_name = input("Enter the name of the model: ")
ft_task_type = input("Enter task type (causal_lm, sequence_classification, token_classification): ")
ft_dataset = input("Enter dataset name or path: ")


ft_agent = Agent(
    role="HuggingFace Fine-Tuning Expert",
    goal=(
        """Act as a top-tier HuggingFace trainer capable of orchestrating the entire fine-tuning pipeline.
        Guide the user to select a valid HuggingFace task type ('causal_lm', 'sequence_classification', or 'token_classification'),
        validate all provided inputs including model_name, dataset_name_or_path, and training arguments,
        auto-correct obvious parameter mistakes, dynamically load the correct transformer model class,
        prepare datasets with optimal tokenization strategies for the task type,
        configure sensible but high-performing defaults for TrainingArguments while honoring user overrides,
        and execute fine-tuning exclusively using the FTTool.
        Return only the exact JSON object provided by FTTool — no extra commentary, formatting, or explanation."""
    ),
    backstory=(
        """You are an elite machine learning architect with deep expertise in HuggingFace Transformers,
        specializing in maximizing training efficiency and model performance.
        Over the years, you’ve orchestrated countless fine-tuning runs across NLP tasks,
        balancing precision engineering with rapid prototyping.
        You are obsessive about parameter validation, preventing wasted GPU cycles,
        and ensuring reproducibility. Your workflow is surgical:
        prompt, validate, adapt, execute, and deliver — with zero noise in the output."""
    ),
    verbose=True,
    llm=llm,
)


ft_task_transformer = Task(
    description=f"""
        Prompt the user to choose the HuggingFace task type they want to perform (options: "causal_lm", "sequence_classification", or "token_classification").
        Once the task type is selected, automatically collect and validate all required parameters:
        a. model_name (string, valid HuggingFace model ID or local path)
        b. dataset_name_or_path (string, valid HuggingFace dataset name or local file path)
        c. task_type (string, must be one of the supported task types)
        d. optional training arguments (learning_rate, num_epochs, train_batch_size, eval_batch_size, max_length, evaluation_strategy, weight_decay, logging_steps).

        Fine-tune the HuggingFace model {ft_model_name} for the task {ft_task_type} using the dataset {ft_dataset}.
        Use the FineTuneTool to configure and run training. Pass model_name='{ft_model_name}, dataset_name_or_path={ft_dataset}, task_type={ft_task_type}, and any additional training arguments.

        After collecting and validating inputs:
        1. Dynamically load the correct model class based on task_type:
            a. causal_lm -> AutoModelForCausalLM
            b. sequence_classification -> AutoModelForSequenceClassification
            c. token_classification -> AutoModelForTokenClassification
        2. Load and tokenize the dataset according to task_type, applying appropriate truncation, padding, and max_length rules.
        3. Automatically configure TrainingArguments with sensible defaults, allowing overrides from user-supplied training arguments.
        4. Run fine-tuning using the FTTool, passing model_name, dataset_name_or_path, task_type, and all additional arguments to `_run()`.
        5. On completion, return **only** the JSON string returned by FTTool containing:
            a. "status" (success or error)
            b. "output_dir" (model save path)
            c. "message" (summary of fine-tuning)
        Do not include any additional text, explanations, or formatting outside the returned JSON.""",
    expected_output="""A single JSON object returned by the FineTuneTool containing exactly these keys:
        model_save_path(string) – the path where the fine-tuned model is stored, training_details(object) – metrics and configuration used during training,and status (string) – the final training status (e.g., 'success', 'failed').
        No extra text, explanations, or formatting outside of the JSON object.""",
    tools=[ft_tool],
    agent=ft_agent,
)


# CREW
crew = Crew(
    agents=[ft_agent],
    tasks=[ft_task_transformer],
    verbose=True,
)


# KICKOFF
result = crew.kickoff()

from transformers import pipeline

model_path = "./output/fine_tuning_results"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

classifier = pipeline("text-classification",
    model=model,
    tokenizer=tokenizer
)

text1 = "Thug life is one of the worst movie of all time"
text2 = "It's the best movie i have ever seen in my life"
result1 = classifier(text1)
result2 = classifier(text2)
print(result1)
print(result2)

